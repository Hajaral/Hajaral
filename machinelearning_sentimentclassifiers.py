# -*- coding: utf-8 -*-
"""MachineLearning_SentimentClassifiers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ECSo3JtEsavMxaoBJ3OOT94FeTo6OkRv

## Building three machine learning models including SVM, NB, and RF using sk-learn Python library to train a model to classify each tweet into positive or negative sentiment
"""

# import libraries 
from google.colab import files
import pandas as pd
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
import numpy as np
from scipy.sparse import hstack
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

# Upload the CSV file
uploaded = files.upload()

data = pd.read_csv("Tweets.csv")

data = data[['text', 'airline_sentiment']]
data.head()

#excluding neutral tweets to create 'two-class' classification task
data = data[data['airline_sentiment'] != 'neutral']
data.head()

"""Pre-processing tweets """

# noisy removal  
data['text'] = data['text'].apply(lambda x: x.lower())
data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

#stopwords removal  
stop = set (stopwords.words ("english"))
def remove_stopwords (text): 
  text = [word.lower () for word in text.split() if word.lower() not in stop]
  return " ".join(text)

data["text"] = data["text"].map(remove_stopwords)

#words vectorization using tf-idf
vectorizer = TfidfVectorizer (ngram_range=(1,2), max_features=2000, min_df=5, max_df=0.8)
tfidf = vectorizer.fit_transform(data.text)

#words vectorization usig BoW
count_vectorizer = CountVectorizer(ngram_range=(1,2)) 
vectorized_data = count_vectorizer.fit_transform(data.text)
indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))

#print(indexed_data)

#tfidf = TfidfVectorizer(ngram_range=(1,2))

def sentiment2target(sentiment):
    return {
        'negative': 0,
        'positive' : 1
    }[sentiment]
targets = data.airline_sentiment.apply(sentiment2target)

data_train, data_test, targets_train, targets_test = train_test_split(tfidf, targets, test_size=0.4, random_state=0) #call tfidf feautres vectores as for BoW we can call 'indexed_data' obj
data_train_index = data_train[:,0]
data_train = data_train[:,1:]
data_test_index = data_test[:,0]
data_test = data_test[:,1:]

"""# SVM Model """

clf = OneVsRestClassifier(svm.SVC(gamma=0.01, C=1, probability=True, class_weight='balanced', kernel='rbf'))
clf_output = clf.fit(data_train, targets_train)

clf_output.score(data_test, targets_test)

precision_score(targets_test, clf.predict(data_test), average='weighted')

f1_score(targets_test, clf.predict(data_test), average='weighted')

recall_score(targets_test, clf.predict(data_test), average='weighted')

print(confusion_matrix(targets_test, clf.predict(data_test)))
print(classification_report(targets_test, clf.predict(data_test)))

"""# NB Model"""

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
NB = MultinomialNB ()
NB_output = NB.fit(data_train, targets_train)

models_NB = []
models_NB.append(MultinomialNB())

for model in models_NB:
    model.fit(data_train, targets_train)
    pred = model.predict(data_test)
    acc = accuracy_score(targets_test, pred)
    print(f"Model {model.__class__.__name__} accuracy on test dataset >> {acc}")

precision_score(targets_test, NB_output.predict(data_test), average='weighted')

recall_score(targets_test, NB_output.predict(data_test), average='weighted')

accuracy_score(targets_test, NB_output.predict(data_test))

f1_score(targets_test, NB_output.predict(data_test), average='weighted')

"""# RF MODEL"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=10, random_state=0, max_features= 'sqrt' , min_samples_split= 4) 
rf_output = rf.fit(data_train, targets_train)

rf.score(data_test, targets_test)

f1_score(targets_test, rf.predict(data_test), average='weighted')

precision_score(targets_test, rf.predict(data_test), average='weighted')

recall_score(targets_test, rf.predict(data_test), average='weighted')